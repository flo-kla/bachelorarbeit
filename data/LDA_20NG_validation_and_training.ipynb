{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flokla/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#dependencies\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pprint import pprint\n",
    "import math\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "# Plotting tools\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "#load training Data\n",
    "with open('../data/Datasets/20NG_short/idocnade/training.csv', newline='') as csvfile1:\n",
    "    data_train = pd.read_csv(csvfile1,  names=[\"Label\", \"Text\"])\n",
    "    train_fold = data_train.dropna(subset=['Text'])   \n",
    "\n",
    "#load validation Data\n",
    "with open('../data/Datasets/20NG_short/idocnade/validation.csv', newline='') as csvfile2:\n",
    "    data_val = pd.read_csv(csvfile2,  names=[\"Label\", \"Text\"])\n",
    "    val_fold = data_val.dropna(subset=['Text'])     \n",
    "    \n",
    "frames = [train_fold,vald_fold]\n",
    "train_data = pd.concat(frames)\n",
    "\n",
    "\n",
    "#load testing Data\n",
    "with open('../data/Datasets/20NG_short/idocnade/test.csv', newline='') as csvfile3:\n",
    "    data_test = pd.read_csv(csvfile3,  names=[\"Label\", \"Text\"])  \n",
    "    test_data = data_test.dropna(subset=['Text'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b7b9088d324cc284afa9ee30ce55c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=198.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#perform cross-validation for alpha, beta, k\n",
    "hyperparameter_results = {'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "alpha = [0.1,0.3,0.5,0.7,1]\n",
    "beta = [0.1,0.3,0.5,0.7,1]\n",
    "\n",
    "\n",
    "#data prep\n",
    "dictionary = gensim.corpora.Dictionary(train_fold[\"Text\"].str.split())\n",
    "corpus_train = [dictionary.doc2bow(text.split()) for index,text in train_fold[\"Text\"].iteritems()]\n",
    "corpus_val = [dictionary.doc2bow(text.split()) for index,text in val_fold[\"Text\"].iteritems()]\n",
    "texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus_val]\n",
    "\n",
    "for k in tqdm(range(2, 200, 1)):\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    lda_model = gensim.models.LdaMulticore(corpus=corpus_train,\n",
    "                                                           id2word = dictionary, \n",
    "                                                           num_topics = k, \n",
    "                                                           alpha=a,\n",
    "                                                           eta=b,\n",
    "                                                           workers=15)\n",
    "                    # Coherence\n",
    "                    coherence_model_small = gensim.models.coherencemodel.CoherenceModel(model=lda_model,\n",
    "                                                                                        topn = 10,\n",
    "                                                                                        texts = texts,\n",
    "                                                                                        coherence='c_v')\n",
    "                    coherence_model_large = gensim.models.coherencemodel.CoherenceModel(model=lda_model,\n",
    "                                                                                        topn = 20,\n",
    "                                                                                        texts = texts,\n",
    "                                                                                        coherence='c_v')\n",
    "                    coherence1 = coherence_model_small.get_coherence()\n",
    "                    coherence2 = coherence_model_large.get_coherence()\n",
    "                    c_v = (coherence1+coherence2)/2\n",
    "                    hyperparameter_results['Topics'].append(k)\n",
    "                    hyperparameter_results['Alpha'].append(a)\n",
    "                    hyperparameter_results['Beta'].append(b)\n",
    "                    hyperparameter_results['Coherence'].append(c_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(hyperparameter_results).to_csv('../LDA/model/20NG_short/hyperparameter_tuning.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save best lda model with optimal alpha beta for k=*\n",
    "dictionary = gensim.corpora.Dictionary(train_data[\"Text\"].str.split())\n",
    "corpus_train = [dictionary.doc2bow(text.split()) for index,text in train_data[\"Text\"].iteritems()]\n",
    "lda_model_k_star = gensim.models.LdaMulticore(corpus=corpus_train,\n",
    "                                                           id2word = dictionary, \n",
    "                                                           num_topics = k, \n",
    "                                                           alpha=a,\n",
    "                                                           eta=b,\n",
    "                                                           workers=15)\n",
    "lda_model_k_star.save('../LDA/model/20NG_short/lda.model_with_optimal_k/lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save best lda model with optimal alpha beta for k=20\n",
    "lda_model_k_20 = gensim.models.LdaMulticore(corpus=corpus_train,\n",
    "                                                           id2word = dictionary, \n",
    "                                                           num_topics = k, \n",
    "                                                           alpha=a,\n",
    "                                                           eta=b,\n",
    "                                                           workers=15)\n",
    "lda_model_k_20.save('../LDA/model/20NG_short/lda.model_with_k20/lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save best lda model with optimal alpha beta for k=50\n",
    "lda_model_k_50 = gensim.models.LdaMulticore(corpus=corpus_train,\n",
    "                                                           id2word = dictionary, \n",
    "                                                           num_topics = k, \n",
    "                                                           alpha=a,\n",
    "                                                           eta=b,\n",
    "                                                           workers=15)\n",
    "lda_model_k_50.save('../LDA/model/20NG_short/lda.model_with_k50/lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save best lda model with optimal alpha beta for k=100\n",
    "#save best lda model with optimal alpha beta for k=50\n",
    "lda_model_k_100 = gensim.models.LdaMulticore(corpus=corpus_train,\n",
    "                                                           id2word = dictionary, \n",
    "                                                           num_topics = k, \n",
    "                                                           alpha=a,\n",
    "                                                           eta=b,\n",
    "                                                           workers=15)\n",
    "lda_model_k_100.save('../LDA/model/20NG_short/lda.model_with_k100/lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save best lda model with optimal alpha beta for k=200\n",
    "#save best lda model with optimal alpha beta for k=50\n",
    "lda_model_k_200 = gensim.models.LdaMulticore(corpus=corpus_train,\n",
    "                                                           id2word = dictionary, \n",
    "                                                           num_topics = k, \n",
    "                                                           alpha=a,\n",
    "                                                           eta=b,\n",
    "                                                           workers=15)\n",
    "lda_model_k_200.save('../LDA/model/20NG_short/lda.model_with_k200/lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coherence for top10 and top20 topics words per topic, average across all topics, save highest and lowest c_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform logistisc regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print some topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save everything in result_log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
